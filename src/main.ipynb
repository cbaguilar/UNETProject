{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIcIUMTYykQL"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLDtwlCfanVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0e6594-e379-4495-ae90-5bb41d2ea7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPoM5v37IRb5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "from torch.cuda import empty_cache\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cbaguilar/UNETProject.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK_69f03PrYp",
        "outputId": "b2b892ff-1bb4-415f-def1-0e1521035cd3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UNETProject'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/65)\u001b[K\rremote: Counting objects:   3% (2/65)\u001b[K\rremote: Counting objects:   4% (3/65)\u001b[K\rremote: Counting objects:   6% (4/65)\u001b[K\rremote: Counting objects:   7% (5/65)\u001b[K\rremote: Counting objects:   9% (6/65)\u001b[K\rremote: Counting objects:  10% (7/65)\u001b[K\rremote: Counting objects:  12% (8/65)\u001b[K\rremote: Counting objects:  13% (9/65)\u001b[K\rremote: Counting objects:  15% (10/65)\u001b[K\rremote: Counting objects:  16% (11/65)\u001b[K\rremote: Counting objects:  18% (12/65)\u001b[K\rremote: Counting objects:  20% (13/65)\u001b[K\rremote: Counting objects:  21% (14/65)\u001b[K\rremote: Counting objects:  23% (15/65)\u001b[K\rremote: Counting objects:  24% (16/65)\u001b[K\rremote: Counting objects:  26% (17/65)\u001b[K\rremote: Counting objects:  27% (18/65)\u001b[K\rremote: Counting objects:  29% (19/65)\u001b[K\rremote: Counting objects:  30% (20/65)\u001b[K\rremote: Counting objects:  32% (21/65)\u001b[K\rremote: Counting objects:  33% (22/65)\u001b[K\rremote: Counting objects:  35% (23/65)\u001b[K\rremote: Counting objects:  36% (24/65)\u001b[K\rremote: Counting objects:  38% (25/65)\u001b[K\rremote: Counting objects:  40% (26/65)\u001b[K\rremote: Counting objects:  41% (27/65)\u001b[K\rremote: Counting objects:  43% (28/65)\u001b[K\rremote: Counting objects:  44% (29/65)\u001b[K\rremote: Counting objects:  46% (30/65)\u001b[K\rremote: Counting objects:  47% (31/65)\u001b[K\rremote: Counting objects:  49% (32/65)\u001b[K\rremote: Counting objects:  50% (33/65)\u001b[K\rremote: Counting objects:  52% (34/65)\u001b[K\rremote: Counting objects:  53% (35/65)\u001b[K\rremote: Counting objects:  55% (36/65)\u001b[K\rremote: Counting objects:  56% (37/65)\u001b[K\rremote: Counting objects:  58% (38/65)\u001b[K\rremote: Counting objects:  60% (39/65)\u001b[K\rremote: Counting objects:  61% (40/65)\u001b[K\rremote: Counting objects:  63% (41/65)\u001b[K\rremote: Counting objects:  64% (42/65)\u001b[K\rremote: Counting objects:  66% (43/65)\u001b[K\rremote: Counting objects:  67% (44/65)\u001b[K\rremote: Counting objects:  69% (45/65)\u001b[K\rremote: Counting objects:  70% (46/65)\u001b[K\rremote: Counting objects:  72% (47/65)\u001b[K\rremote: Counting objects:  73% (48/65)\u001b[K\rremote: Counting objects:  75% (49/65)\u001b[K\rremote: Counting objects:  76% (50/65)\u001b[K\rremote: Counting objects:  78% (51/65)\u001b[K\rremote: Counting objects:  80% (52/65)\u001b[K\rremote: Counting objects:  81% (53/65)\u001b[K\rremote: Counting objects:  83% (54/65)\u001b[K\rremote: Counting objects:  84% (55/65)\u001b[K\rremote: Counting objects:  86% (56/65)\u001b[K\rremote: Counting objects:  87% (57/65)\u001b[K\rremote: Counting objects:  89% (58/65)\u001b[K\rremote: Counting objects:  90% (59/65)\u001b[K\rremote: Counting objects:  92% (60/65)\u001b[K\rremote: Counting objects:  93% (61/65)\u001b[K\rremote: Counting objects:  95% (62/65)\u001b[K\rremote: Counting objects:  96% (63/65)\u001b[K\rremote: Counting objects:  98% (64/65)\u001b[K\rremote: Counting objects: 100% (65/65)\u001b[K\rremote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 65 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (65/65), 743.59 KiB | 8.75 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OxA2TdabqvCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e63198-5c46-4c1d-f0c3-fe87ca6bf095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UNETProject/src\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd '/content/UNETProject/src'\n",
        "from models import *\n",
        "from loss import *\n",
        "from utils.pre_processing import *\n",
        "from dataset import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wlImg04Fm69"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DZz9CTePpejp"
      },
      "outputs": [],
      "source": [
        "class Averagvalue(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy_check(mask, prediction):\n",
        "    ims = [mask, prediction]\n",
        "    np_ims = []\n",
        "    for item in ims:\n",
        "        if 'PIL' in str(type(item)):\n",
        "            item = np.array(item)\n",
        "        elif 'torch' in str(type(item)):\n",
        "            item = item.cpu().detach().numpy()\n",
        "        np_ims.append(item)\n",
        "    compare = np.equal(np.where(np_ims[0] > 0.5, 1, 0), np_ims[1])\n",
        "    accuracy = np.sum(compare)\n",
        "    return accuracy / len(np_ims[0].flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uJS-TXBopejp"
      },
      "outputs": [],
      "source": [
        "def BCELoss(prediction, label):\n",
        "    masks_probs_flat = prediction.view(-1)\n",
        "    true_masks_flat = label.float().view(-1)\n",
        "    loss = nn.BCELoss()(masks_probs_flat, true_masks_flat)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ueE9KE8AKHts"
      },
      "outputs": [],
      "source": [
        "def load_hdf5(infile):\n",
        "    with h5py.File(infile, \"r\") as f:  # \"with\" close the file after its nested commands\n",
        "        return f[\"image\"][()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KlVAsnowGtJo"
      },
      "outputs": [],
      "source": [
        "def extract_random_patches(full_imgs, full_masks, patch_h, patch_w, N_patches_per_img):\n",
        "\n",
        "    assert (len(full_imgs.shape) == 4 and len(full_masks.shape) == 4)\n",
        "    assert (full_imgs.shape[0] == full_masks.shape[0] and full_imgs.shape[2] == full_masks.shape[2] and full_imgs.shape[3] == full_masks.shape[3])\n",
        "    assert (full_imgs.shape[1] == 1 or full_imgs.shape[1] == 3)\n",
        "    assert (full_masks.shape[1] == 1)\n",
        "\n",
        "    N_images, N_channels = full_imgs.shape[0], full_imgs.shape[1]\n",
        "    img_h, img_w = full_imgs.shape[2], full_imgs.shape[3]\n",
        "    patches_imgs = np.empty((N_images * N_patches_per_img, N_channels, patch_h, patch_w))\n",
        "    patches_masks = np.empty((N_images * N_patches_per_img, N_channels, patch_h, patch_w))\n",
        "\n",
        "    i_patch = 0\n",
        "    for i in range(N_images):\n",
        "        k = 0\n",
        "        while k < N_patches_per_img:\n",
        "            x_center = np.random.randint(low=int(patch_w / 2), high=img_w - int(patch_w / 2))\n",
        "            y_center = np.random.randint(low=int(patch_h / 2), high=img_h - int(patch_h / 2))\n",
        "            patch_img = full_imgs[i, :, y_center - int(patch_h / 2):y_center + int(patch_h / 2), x_center - int(patch_w / 2):x_center + int(patch_w / 2)]\n",
        "            patch_mask = full_masks[i, :, y_center - int(patch_h / 2):y_center + int(patch_h / 2), x_center - int(patch_w / 2):x_center + int(patch_w / 2)]\n",
        "            patches_imgs[i_patch] = patch_img\n",
        "            patches_masks[i_patch] = patch_mask\n",
        "            i_patch += 1\n",
        "            k += 1\n",
        "\n",
        "    return patches_imgs, patches_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lwQEAjKKFue2"
      },
      "outputs": [],
      "source": [
        "def get_data_training(train_imgs_original, train_masks, patch_height, patch_width, N_patches_per_image):\n",
        "    train_imgs = my_PreProc(train_imgs_original)\n",
        "    train_masks = train_masks / 255.0\n",
        "    assert (np.min(train_masks) == 0 and np.max(train_masks) == 1)\n",
        "\n",
        "    patches_imgs_train, patches_masks_train = extract_random_patches(train_imgs, train_masks, patch_height, patch_width, N_patches_per_image)\n",
        "    return patches_imgs_train, patches_masks_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htUnP983ygFK"
      },
      "source": [
        "# Train and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBH3u_8gpejq"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, epoch):\n",
        "    model.eval()\n",
        "    epoch_time = Averagvalue()\n",
        "    losses = Averagvalue()\n",
        "    acc = Averagvalue()\n",
        "    end = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (image, mask) in enumerate(test_loader):\n",
        "            image = image.float().to(device).requires_grad_(False)\n",
        "            mask = mask.to(device).requires_grad_(False)\n",
        "            pred = model(image)\n",
        "\n",
        "            loss = loss_fn(pred, mask)\n",
        "            # loss_fn(pred, onehot(mask, 2).cuda())\n",
        "            accuracy = accuracy_check(pred, mask)\n",
        "            acc.update(accuracy, 1)\n",
        "\n",
        "            losses.update(loss.item(), image.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "    epoch_time.update(time.time() - end)\n",
        "    info = 'TEST Epoch: [{0}/{1}]'.format(epoch, N_epochs) + \\\n",
        "           'Test Epoch Time {batch_time.val:.3f} (avg:{batch_time.avg:.3f}) '.format(batch_time=epoch_time) + \\\n",
        "           'Acc {acc.val:f} (avg:{acc.avg:f}) '.format(acc=acc) + \\\n",
        "           'Loss {loss.val:f} (avg:{loss.avg:f}) '.format(loss=losses)\n",
        "    print(info)\n",
        "    return losses.avg, acc.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl6A5vy9pejq"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    losses = Averagvalue()\n",
        "    acc = Averagvalue()\n",
        "    optimizer.zero_grad()\n",
        "    for i, (image, mask) in enumerate(train_loader):\n",
        "        image = image.float().to(device)\n",
        "        mask = mask.float().to(device)\n",
        "        pred = model(image)\n",
        "\n",
        "        loss = loss_fn(pred, mask)\n",
        "        # loss = loss_fn(pred, onehot(mask, 2).cuda())\n",
        "        accuracy = accuracy_check(pred, mask)\n",
        "        acc.update(accuracy, 1)\n",
        "\n",
        "        losses.update(loss.item(), image.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return losses.avg, acc.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZA3eY_3yiZG"
      },
      "source": [
        "# Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRs_VRtF8nK1"
      },
      "outputs": [],
      "source": [
        "def prepare_model(patches_per_image):\n",
        "\n",
        "  patches_imgs_train, patches_masks_train = get_data_training(\n",
        "      train_imgs_original=load_hdf5(os.path.join(GOOGLE_DRIVE_PATH, \"DRIVE_dataset/DRIVE_imgs_train.hdf5\")),\n",
        "      train_masks=load_hdf5(os.path.join(GOOGLE_DRIVE_PATH, \"DRIVE_dataset/DRIVE_groundTruth_train.hdf5\")),\n",
        "      patch_height=48,\n",
        "      patch_width=48,\n",
        "      N_patches_per_image=patches_per_image\n",
        "  )\n",
        "\n",
        "  patches_imgs_train = np.transpose(patches_imgs_train, (0, 2, 3, 1))\n",
        "  patches_masks_train = np.transpose(patches_masks_train, (0, 2, 3, 1))\n",
        "  training_data = vessel_dataset(patches_imgs_train, patches_masks_train, 0.9, split='train')\n",
        "  validation_data = vessel_dataset(patches_imgs_train, patches_masks_train, 0.9, split='val')\n",
        "\n",
        "  training_loader = torch.utils.data.DataLoader(training_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "  validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=1, shuffle=False, num_workers=1)\n",
        "  return training_loader, validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpORJWtaEIEZ",
        "outputId": "a87259e6-66ca-4e99-de91-ef48859ef905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_patches: 1 EPOCH 0 | Training Loss: 0.8821908632914225, Training Accuracy: 0.8472873263888889\n",
            "num_patches: 1 EPOCH 1 | Training Loss: 0.8806025054719713, Training Accuracy: 0.8070529513888889\n",
            "num_patches: 1 EPOCH 2 | Training Loss: 0.8795546690622965, Training Accuracy: 0.7636935763888888\n",
            "num_patches: 1 EPOCH 3 | Training Loss: 0.8786631557676527, Training Accuracy: 0.7790798611111109\n",
            "num_patches: 1 EPOCH 4 | Training Loss: 0.8776960372924805, Training Accuracy: 0.7408637152777777\n",
            "num_patches: 1 EPOCH 5 | Training Loss: 0.8770704401863946, Training Accuracy: 0.7509765625000001\n",
            "num_patches: 1 EPOCH 6 | Training Loss: 0.8760446310043335, Training Accuracy: 0.7361328125\n",
            "num_patches: 1 EPOCH 7 | Training Loss: 0.8746367560492622, Training Accuracy: 0.7060763888888888\n",
            "num_patches: 1 EPOCH 8 | Training Loss: 0.8743447462717692, Training Accuracy: 0.6087456597222223\n",
            "num_patches: 1 EPOCH 9 | Training Loss: 0.8731250431802537, Training Accuracy: 0.5693793402777778\n",
            "num_patches: 4 EPOCH 0 | Training Loss: 0.8276518285274506, Training Accuracy: 0.28471016589506176\n",
            "num_patches: 4 EPOCH 1 | Training Loss: 0.8241612745655907, Training Accuracy: 0.31421802662037046\n",
            "num_patches: 4 EPOCH 2 | Training Loss: 0.820601052708096, Training Accuracy: 0.3372214988425926\n",
            "num_patches: 4 EPOCH 3 | Training Loss: 0.8173041178120507, Training Accuracy: 0.3722933545524691\n",
            "num_patches: 4 EPOCH 4 | Training Loss: 0.8111169868045383, Training Accuracy: 0.3837468653549383\n",
            "num_patches: 4 EPOCH 5 | Training Loss: 0.803767012225257, Training Accuracy: 0.41776982060185186\n",
            "num_patches: 4 EPOCH 6 | Training Loss: 0.7941025329960717, Training Accuracy: 0.4362220293209877\n",
            "num_patches: 4 EPOCH 7 | Training Loss: 0.783809201584922, Training Accuracy: 0.46542245370370366\n",
            "num_patches: 4 EPOCH 8 | Training Loss: 0.7741397950384352, Training Accuracy: 0.5001326195987654\n",
            "num_patches: 4 EPOCH 9 | Training Loss: 0.7671496470769247, Training Accuracy: 0.5164689429012346\n",
            "num_patches: 16 EPOCH 0 | Training Loss: 0.8461095268527666, Training Accuracy: 0.5902958622685189\n",
            "num_patches: 16 EPOCH 1 | Training Loss: 0.8414052418536611, Training Accuracy: 0.446287555459105\n",
            "num_patches: 16 EPOCH 2 | Training Loss: 0.8219233146972127, Training Accuracy: 0.5014753930362654\n",
            "num_patches: 16 EPOCH 3 | Training Loss: 0.7849029410216544, Training Accuracy: 0.5460762683256172\n",
            "num_patches: 16 EPOCH 4 | Training Loss: 0.7650011637144618, Training Accuracy: 0.5713176962770059\n",
            "num_patches: 16 EPOCH 5 | Training Loss: 0.754074407948388, Training Accuracy: 0.6472936559606481\n",
            "num_patches: 16 EPOCH 6 | Training Loss: 0.7424465599987242, Training Accuracy: 0.6765784746334877\n",
            "num_patches: 16 EPOCH 7 | Training Loss: 0.7366938375764422, Training Accuracy: 0.6894651813271608\n",
            "num_patches: 16 EPOCH 8 | Training Loss: 0.7309747611482939, Training Accuracy: 0.6918327425733025\n",
            "num_patches: 16 EPOCH 9 | Training Loss: 0.7250354960560799, Training Accuracy: 0.7462444540895065\n",
            "num_patches: 64 EPOCH 0 | Training Loss: 0.8289301821755038, Training Accuracy: 0.2365782937885803\n",
            "num_patches: 64 EPOCH 1 | Training Loss: 0.7694587978637881, Training Accuracy: 0.48752659927179803\n",
            "num_patches: 64 EPOCH 2 | Training Loss: 0.7412028821806113, Training Accuracy: 0.5183836383584103\n",
            "num_patches: 64 EPOCH 3 | Training Loss: 0.72225796783136, Training Accuracy: 0.5373919451678241\n",
            "num_patches: 64 EPOCH 4 | Training Loss: 0.7027583959408932, Training Accuracy: 0.5531710400993444\n",
            "num_patches: 64 EPOCH 5 | Training Loss: 0.6868012282583449, Training Accuracy: 0.5460457507474918\n",
            "num_patches: 64 EPOCH 6 | Training Loss: 0.6753093466783563, Training Accuracy: 0.5540036801938657\n",
            "num_patches: 64 EPOCH 7 | Training Loss: 0.6591113293543458, Training Accuracy: 0.5780790352527001\n",
            "num_patches: 64 EPOCH 8 | Training Loss: 0.6451996949811777, Training Accuracy: 0.5993814350646218\n",
            "num_patches: 64 EPOCH 9 | Training Loss: 0.6320620245403714, Training Accuracy: 0.613351327401621\n",
            "num_patches: 256 EPOCH 0 | Training Loss: 0.7532365956447191, Training Accuracy: 0.6614105789749712\n",
            "num_patches: 256 EPOCH 1 | Training Loss: 0.6774929110219495, Training Accuracy: 0.7967234481999907\n",
            "num_patches: 256 EPOCH 2 | Training Loss: 0.6305631784101328, Training Accuracy: 0.8053286517107934\n",
            "num_patches: 256 EPOCH 3 | Training Loss: 0.592492354180043, Training Accuracy: 0.8181509324062014\n",
            "num_patches: 256 EPOCH 4 | Training Loss: 0.5618707216862175, Training Accuracy: 0.8290654877085728\n",
            "num_patches: 256 EPOCH 5 | Training Loss: 0.5381654927817484, Training Accuracy: 0.8483437432183158\n",
            "num_patches: 256 EPOCH 6 | Training Loss: 0.5209455420392461, Training Accuracy: 0.8588397179120852\n",
            "num_patches: 256 EPOCH 7 | Training Loss: 0.5095145552962398, Training Accuracy: 0.8789260299117475\n",
            "num_patches: 256 EPOCH 8 | Training Loss: 0.4972098295628611, Training Accuracy: 0.8950739731023349\n",
            "num_patches: 256 EPOCH 9 | Training Loss: 0.47627472986156744, Training Accuracy: 0.9271668799129534\n",
            "num_patches: 1024 EPOCH 0 | Training Loss: 0.6562353143300343, Training Accuracy: 0.684015485975478\n",
            "num_patches: 1024 EPOCH 1 | Training Loss: 0.5346180208014428, Training Accuracy: 0.8492269162778492\n",
            "num_patches: 1024 EPOCH 2 | Training Loss: 0.4752292140652167, Training Accuracy: 0.9202328199221778\n",
            "num_patches: 1024 EPOCH 3 | Training Loss: 0.4458023235492874, Training Accuracy: 0.9431830983103069\n",
            "num_patches: 1024 EPOCH 4 | Training Loss: 0.4342324665404804, Training Accuracy: 0.9466416629744123\n",
            "num_patches: 1024 EPOCH 5 | Training Loss: 0.4273064409951783, Training Accuracy: 0.9484677020414369\n",
            "num_patches: 1024 EPOCH 6 | Training Loss: 0.4217260857953483, Training Accuracy: 0.94945905238022\n",
            "num_patches: 1024 EPOCH 7 | Training Loss: 0.416503832187219, Training Accuracy: 0.95118894694764\n",
            "num_patches: 1024 EPOCH 8 | Training Loss: 0.4120334672657514, Training Accuracy: 0.9524920192765585\n",
            "num_patches: 1024 EPOCH 9 | Training Loss: 0.40874441469269285, Training Accuracy: 0.9530129844759713\n"
          ]
        }
      ],
      "source": [
        "nums = [1, 4, 16, 64, 256, 1024]\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "\n",
        "for num in nums:\n",
        "  training_loader, validation_loader = prepare_model(num)\n",
        "  mymodel = DUNetV1V2(n_channels=1, n_classes=1).to(device)\n",
        "  optimizer = torch.optim.Adam(mymodel.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "  # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=12)\n",
        "  # loss_fn = nn.BCELoss()\n",
        "  loss_fn = DiceLoss(p=1)\n",
        "\n",
        "  N_epochs = 10\n",
        "\n",
        "  for epoch in range(N_epochs):\n",
        "      train_loss, train_acc = train(mymodel, training_loader, optimizer, epoch)\n",
        "      print(f\"num_patches: {num} EPOCH {epoch} | Training Loss: {train_loss}, Training Accuracy: {train_acc}\")\n",
        "      filename = os.path.join(GOOGLE_DRIVE_PATH, 'checkpoints', 'checkpoint_epoch_%03d.pth' % (epoch + 1))\n",
        "      torch.save({\n",
        "          'num_patches': num,\n",
        "          'epoch': epoch + 1,\n",
        "          'state_dict': mymodel.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()\n",
        "      }, filename)\n",
        "      # val_loss, val_acc = test(mymodel, validation_loader, epoch)\n",
        "      # scheduler.step(val_loss)\n",
        "      # print(f\"EPOCH {epoch} | Validation Accuracy: {val_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnrWWGOSyRAt"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiLXUx1VDf_p"
      },
      "outputs": [],
      "source": [
        "# loaded_model = DUNetV1V2(n_channels=1, n_classes=1)\n",
        "# loaded_model.load_state_dict(torch.load('/content/drive/MyDrive/dunet/checkpoints/DRIVE/checkpoint_epoch_010.pth')['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4dhV-aTpBYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5f3aee-cfef-4156-c63c-c49deb4e1eb9",
        "id": "0sMtlY02pBxf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_func0 num_patches: 512 EPOCH 0 | Training Loss: 0.725548972750807, Training Accuracy: 0.5199940057448392\n",
            "loss_func0 num_patches: 512 EPOCH 1 | Training Loss: 0.6083346890647792, Training Accuracy: 0.6632574104968415\n",
            "loss_func0 num_patches: 512 EPOCH 2 | Training Loss: 0.5515872061110308, Training Accuracy: 0.8126370936264231\n",
            "loss_func0 num_patches: 512 EPOCH 3 | Training Loss: 0.5225636076841814, Training Accuracy: 0.8486565483940973\n",
            "loss_func0 num_patches: 512 EPOCH 4 | Training Loss: 0.49509189144009724, Training Accuracy: 0.9057488618073641\n",
            "loss_func0 num_patches: 512 EPOCH 5 | Training Loss: 0.46564590681939283, Training Accuracy: 0.9407444706669573\n",
            "loss_func0 num_patches: 512 EPOCH 6 | Training Loss: 0.4546026534905347, Training Accuracy: 0.9449512340404402\n",
            "loss_func0 num_patches: 512 EPOCH 7 | Training Loss: 0.4479232372185733, Training Accuracy: 0.9461669921875012\n",
            "loss_func0 num_patches: 512 EPOCH 8 | Training Loss: 0.44217337805053425, Training Accuracy: 0.9472185299720276\n",
            "loss_func0 num_patches: 512 EPOCH 9 | Training Loss: 0.43862972101972747, Training Accuracy: 0.9477573912820682\n",
            "loss_func1 num_patches: 512 EPOCH 0 | Training Loss: 0.7033583840820938, Training Accuracy: 0.7555548114541117\n",
            "loss_func1 num_patches: 512 EPOCH 1 | Training Loss: 0.5925104345790007, Training Accuracy: 0.8351120654447567\n",
            "loss_func1 num_patches: 512 EPOCH 2 | Training Loss: 0.534157551618086, Training Accuracy: 0.8683764139811192\n",
            "loss_func1 num_patches: 512 EPOCH 3 | Training Loss: 0.5076167793789258, Training Accuracy: 0.8935793652946555\n",
            "loss_func1 num_patches: 512 EPOCH 4 | Training Loss: 0.4753187268959462, Training Accuracy: 0.9253571592731239\n",
            "loss_func1 num_patches: 512 EPOCH 5 | Training Loss: 0.44815866732581827, Training Accuracy: 0.9440517190061972\n",
            "loss_func1 num_patches: 512 EPOCH 6 | Training Loss: 0.436720582365524, Training Accuracy: 0.9475492783534674\n",
            "loss_func1 num_patches: 512 EPOCH 7 | Training Loss: 0.4301479380875308, Training Accuracy: 0.9486586488323433\n",
            "loss_func1 num_patches: 512 EPOCH 8 | Training Loss: 0.4231488810489989, Training Accuracy: 0.9499571529435537\n",
            "loss_func1 num_patches: 512 EPOCH 9 | Training Loss: 0.41888014803407714, Training Accuracy: 0.950777407045718\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "loss_funcs = [nn.BCELoss(), DiceLoss(p=1)]\n",
        "\n",
        "for i in range(2):\n",
        "  global loss_func\n",
        "  loss_func = loss_funcs[i]\n",
        "  num=512\n",
        "  training_loader, validation_loader = prepare_model(num)\n",
        "  mymodel = DUNetV1V2(n_channels=1, n_classes=1).to(device)\n",
        "  optimizer = torch.optim.Adam(mymodel.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "  # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=12)\n",
        "  # loss_fn = nn.BCELoss()\n",
        "  # loss_fn = DiceLoss(p=1)\n",
        "\n",
        "  N_epochs = 10\n",
        "\n",
        "  for epoch in range(N_epochs):\n",
        "      train_loss, train_acc = train(mymodel, training_loader, optimizer, epoch)\n",
        "      print(f\"loss_func{i} num_patches: {num} EPOCH {epoch} | Training Loss: {train_loss}, Training Accuracy: {train_acc}\")\n",
        "      filename = os.path.join(GOOGLE_DRIVE_PATH, 'checkpoints', 'checkpoint_epoch_%03d.pth' % (epoch + 1))\n",
        "      torch.save({\n",
        "          'num_patches': num,\n",
        "          'epoch': epoch + 1,\n",
        "          'state_dict': mymodel.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()\n",
        "      }, filename)\n",
        "      # val_loss, val_acc = test(mymodel, validation_loader, epoch)\n",
        "      # scheduler.step(val_loss)\n",
        "      # print(f\"EPOCH {epoch} | Validation Accuracy: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3WW-OkUm7cU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "2e7f31ab-5b6c-499f-98a3-fb5095d9f08c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/content/drive/MyDrive/dunet/DRIVE_dataset/DRIVE_imgs_test.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f3650cf12d9f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_imgs_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/dunet/DRIVE_dataset/DRIVE_imgs_test.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_PreProc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/donut/utils/help_functions.py\u001b[0m in \u001b[0;36mload_hdf5\u001b[0;34m(infile)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# \"with\" close the file after its nested commands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/content/drive/MyDrive/dunet/DRIVE_dataset/DRIVE_imgs_test.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "test_imgs_original = load_hdf5(\"/content/drive/MyDrive/dunet/DRIVE_dataset/DRIVE_imgs_test.hdf5\")\n",
        "\n",
        "test_imgs = my_PreProc(test_imgs_original)\n",
        "test_imgs = np.transpose(test_imgs, (0, 2, 3, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLLad954Dksp"
      },
      "source": [
        "### BCELoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oLC6jsRF_F2"
      },
      "outputs": [],
      "source": [
        "sample_image = transforms.ToTensor()(test_imgs[3])\n",
        "mymodel.eval()\n",
        "with torch.no_grad():\n",
        "  sample_pred = mymodel(sample_image.float().unsqueeze(0).to(device))\n",
        "sample_pred = sample_pred.squeeze().detach().cpu().numpy()\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(sample_image.squeeze(), cmap='gray')\n",
        "ax[1].imshow(sample_pred, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VnyTyDhDmTS"
      },
      "source": [
        "### Dice Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1BndHYH3L9V"
      },
      "outputs": [],
      "source": [
        "sample_image = transforms.ToTensor()(test_imgs[3])\n",
        "mymodel.eval()\n",
        "with torch.no_grad():\n",
        "  sample_pred = mymodel(sample_image.float().unsqueeze(0).to(device))\n",
        "sample_pred = sample_pred.squeeze().detach().cpu().numpy()\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(sample_image.squeeze(), cmap='gray')\n",
        "ax[1].imshow(sample_pred, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiKhJQ8WpddB"
      },
      "source": [
        "# Lesions Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGTFHgnqpejq"
      },
      "outputs": [],
      "source": [
        "patches_imgs_train, patches_masks_train = get_data_training(\n",
        "    train_imgs_original=load_hdf5(os.path.join(GOOGLE_DRIVE_PATH, \"BIGDATA_dataset/BIGDATA_imgs_train.hdf5\")),\n",
        "    train_masks=load_hdf5(os.path.join(GOOGLE_DRIVE_PATH, \"BIGDATA_dataset/BIGDATA_groundTruth_train.hdf5\")),\n",
        "    patch_height=64,\n",
        "    patch_width=64,\n",
        "    N_patches_per_image=500\n",
        ")\n",
        "\n",
        "patches_imgs_train = np.transpose(patches_imgs_train, (0, 2, 3, 1))\n",
        "patches_masks_train = np.transpose(patches_masks_train, (0, 2, 3, 1))\n",
        "training_data = vessel_dataset(patches_imgs_train, patches_masks_train, 0.9, split='train')\n",
        "validation_data = vessel_dataset(patches_imgs_train, patches_masks_train, 0.9, split='val')\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=1, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yPZ3gqQxoZG"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "mymodel = DUNetV1V2(n_channels=1, n_classes=1).to(device)\n",
        "optimizer = torch.optim.Adam(mymodel.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=12)\n",
        "loss_fn = nn.BCELoss()\n",
        "# loss_fn = DiceLoss(p=1)\n",
        "\n",
        "N_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykjAtmbcxjFl"
      },
      "outputs": [],
      "source": [
        "for epoch in range(N_epochs):\n",
        "    train_loss, train_acc = train(mymodel, training_loader, optimizer, epoch)\n",
        "    print(f\"EPOCH {epoch} | Training Loss: {train_loss}, Training Accuracy: {train_acc}\")\n",
        "    filename = os.path.join(GOOGLE_DRIVE_PATH, 'checkpoints', 'checkpoint_epoch_%03d.pth' % (epoch + 1))\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': mymodel.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, filename)\n",
        "    # val_loss, val_acc = test(mymodel, validation_loader, epoch)\n",
        "    # scheduler.step(val_loss)\n",
        "    # print(f\"EPOCH {epoch} | Validation Accuracy: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVTdfVc6wVlp"
      },
      "outputs": [],
      "source": [
        "loaded_model = DUNetV1V2(n_channels=1, n_classes=1)\n",
        "loaded_model.load_state_dict(torch.load('/content/drive/MyDrive/dunet/checkpoints/BIGDATA/checkpoint_epoch_010.pth')['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_imgs_original = load_hdf5(\"/content/drive/MyDrive/dunet/BIGDATA_dataset/BIGDATA_imgs_test.hdf5\")\n",
        "\n",
        "test_imgs = my_PreProc(test_imgs_original)\n",
        "test_imgs = np.transpose(test_imgs, (0, 2, 3, 1))"
      ],
      "metadata": {
        "id": "QFNIHZK_xAwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = transforms.ToTensor()(test_imgs[10])\n",
        "loaded_model.eval()\n",
        "with torch.no_grad():\n",
        "  sample_pred = loaded_model(sample_image.float().unsqueeze(0))\n",
        "sample_pred = sample_pred.squeeze().detach().cpu().numpy()\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(sample_image.squeeze(), cmap='gray')\n",
        "ax[1].imshow(sample_pred, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCfMt6EVwmYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLZSjR5n1A4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}